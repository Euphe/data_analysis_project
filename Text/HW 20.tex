\documentclass[a4paper,14pt]{article}

%\includeonly{topics/t2,topics/t3} % компилировать только указанные главы
%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\frenchspacing
%\usepackage{fontspec} 
%\setmainfont[Ligatures={TeX,Historic}]{Times New Roman}

%%%\usepackage{fancyhdr} % {\tiny  }Колонтитулы 
%\pagestyle{fancy} 
%\renewcommand{\headrulewidth}{0mm} % Толщина линейки, отчеркивающей верхний колонтитул 
%\lfoot{Нижний левый} 
%\rfoot{Нижний правый} 
%\rhead{Верхний правый} 
%\chead{Верхний в центре} 
%\lhead{Верхний левый} 
% \cfoot{Нижний в центре} % По умолчанию здесь номер страницы 

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
%\usepackage{ dsfont } % more math fonts!
%\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
%\renewcommand{\phi}{\ensuremath{\varphi}}
%\renewcommand{\kappa}{\ensuremath{\varkappa}}
%\renewcommand{\le}{\ensuremath{\leqslant}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
%\renewcommand{\ge}{\ensuremath{\geqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
%\renewcommand{\emptyset}{\varnothing}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумереация формул слева

%%% Работа с картинками
\usepackage{graphicx}  % Для вставки рисунков
\graphicspath{crimeimages/}  % папки с картинками
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом








%%% Работа с таблицами
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
	{\hbox{$\mathsurround=0pt #1$}}{}}

\newcommand{\source}[1]{\small{Источник: #1}}

%%% Страница
\usepackage{extsizes} % Возможность сделать 14-й шрифт
\usepackage{geometry} % Простой способ задавать поля
\geometry{top=20mm}
\geometry{bottom=20mm}
\geometry{left=35mm}
\geometry{right=15mm}
%
%\usepackage{fancyhdr} % Колонтитулы
% 	\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
% 	\lfoot{Нижний левый}
% 	\rfoot{Нижний правый}
% 	\rhead{Верхний правый}
% 	\chead{Верхний в центре}
% 	\lhead{Верхний левый}
%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы

\usepackage{setspace} % Интерлиньяж
\onehalfspacing % Интерлиньяж 1.5
%\doublespacing % Интерлиньяж 2
%\singlespacing % Интерлиньяж 1

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
%\usepackage{soul} % Модификаторы начертания

\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
\hypersetup{				% Гиперссылки
	unicode=true,           % русские буквы в раздела PDF
	%	pdftitle={Заголовок},   % Заголовок
	%	pdfauthor={Автор},      % Автор
	%	pdfsubject={Тема},      % Тема
	%	pdfcreator={Создатель}, % Создатель
	%	pdfproducer={Производитель}, % Производитель
	%	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
	colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
	linkcolor=blue,          % внутренние ссылки
	citecolor=blue,        % на библиографию
	filecolor=magenta,      % на файлы
	urlcolor=ForestGreen           % на URL
}


% \usepackage{csquotes} % Еще инструменты для ссылок
%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

%%% Программирование
\usepackage{etoolbox} % логические операторы
%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}

\usepackage{tikz} % Работа с графикой
% \usepackage{pgfplots}
% \usepackage{pgfplotstable}

% % % Для работы с report
%\renewcommand{\chaptername}{Глава}


\usepackage{cite} % Работа с библиографией
%\usepackage[superscript]{cite} % Ссылки в верхних индексах
%\usepackage[nocompress]{cite} % 
%\usepackage{csquotes} % Еще инструменты для ссылок

% \usepackage{natbib} %библиография плюс https://ru.wikibooks.org/wiki/LaTeX/%D0%A3%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%B5%D0%B9
%\bibliographystyle{rmpaps} 


%%%% Работа со списками
%\usepackage{enumitem} %%% дополнительная работа со списками. !а может и неправильно написано — гугли
\usepackage{paralist}										% compact lists

\renewcommand{\citeleft}{(}
\renewcommand{\citeright}{)}

\usepackage{adjustbox} 
\addto\captionsrussian{% Replace "english" with the language you use
	\renewcommand{\contentsname}%
	{Table Of Contents}%
}

\author{Yulia Gurova}
\title{Tax systems}
















   % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
  %  \usepackage{upquote} % Upright quotes for verbatim code
%    \usepackage{eurosym} % defines \euro
%    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
%    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
   \usepackage{longtable} % longtable support required by pandoc >1.10
   \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{hw2}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}


	\thispagestyle{empty}    % +1 - это титульный лист
	
	\begin{center}
		
		
		THE RUSSIAN GOVERNMENT \\
		FEDERAL STATE AUTONOMUS EDUCATIONAL INSTITUTION \\ FOR HIGHER PROFESSIONAL EDUCATION \\ NATIONAL RESEARCH UNIVERSITY \\ ``HIGHER SCHOOL OF ECONOMICS''
		
		\large
		\vspace{2 cm}
		%	\textsf{
		%	Факультет экономических наук
		%	\\ Образовательная программа «Экономика» %}           %ТЕКСТ БЕЗ ЗАСЕЧЕК
	\end{center}
	
	\vspace{2 cm}
	\begin{center}
		%	\vspace{13ex}
		\vspace{1 cm} \textbf{The Analysis of On-screen Movie Kills} \\ \vspace{0.5 cm} Homework Project 2018/2019 
		
		
	\end{center}
	
	\vspace{2 cm}
	
	\begin{flushright}
		%	\noindent
		{ \textbf{The team:} \\ Boris Tseitlin \\ Konstantin Romashchenko \\ Yulia Gurova}
		
		\vspace{1 cm}
		
		{ \textbf{MSc Program “Data Science” } \\ 1$^{st}$ year \\
			Faculty of Computer Science }
	\end{flushright}
	
	\begin{center}
		\vfill
		Moscow 2018	
	\end{center}
	
	\newpage
	\tableofcontents
	\newpage
	
	
	\section{THE CHOICE OF THE DATASET}

    The dataset that is used for the project contains information about  on-screen deaths in movies. There are 545 movies (more then 100 objects) and 8 characteristics including names, so the dataset meets the requirements. 
    
    The sourse for the data is the thematic web-site \href{http://www.moviebodycounts.com/}{moviebodycounts.com}. This dataset was processed and published on \href{https://figshare.com/articles/On_screen_movie_kill_counts_for_hundreds_of_films/889719}{figshare.com}. It was gathered  in accordance with the \href{http://moviebodycounts.proboards.com/thread/6}{rules}, which are published on the web-site. We took several characteristics for the consideration: the release year of the film, MPAA rating (Motion Picture Association of America film rating system), genre or genres, the name of the director, the lenth of the film in minutes,  IMDB rating based on user ratings. The main feature that we consider is the number of on-screen deaths in the movie. 
    
    The analysis of the data may reveal how the ratings depend on the number of deaths, how this number relates to the release year and so on. Moreover, genre and length of the film combined with on-screen violence may provide information on age ratings. This is a good set for the classification and clustering problems, as the films are grouped by genres and MPAA ratings. Movies in these groups are similar inside the groups, but dissimilar between them. 
    
    This analysis may be the first step to the automation of age rating systems. Moreover it may be helpful in the development of recommendation systems. And, as watching movies is the common interest of our team, the work with the dataset will inspire us to further conquest in Data Analysis.
    
    
    \newpage
    
    \section{K-means clustering}
    
 For this part of the task we choose three quantitative features:\textit{ Body\_Count}, the number of on-screen deaths in the movie; \textit{Length\_Minutes}, the length in minutes; \textit{Year}, the year of the release. These are quantitive features that, by our hypotheses, may be useful in cluster analysis of the dataset.
 
 \subsection{Preprocessing}
 First we standardize the dataset. We center by mean and normalize by range:
 
 \footnotesize
 \begin{Verbatim}[commandchars=\\\{\}]
 {\color{incolor}In [{\color{incolor}11}]:}  \PY{k}{def} \PY{n+nf}{normalize}\PY{p}{(}\PY{n}{vec}\PY{p}{)}\PY{p}{:}
 \PY{k}{\ \ \ \ return} \PY{p}{(}\PY{n}{vec} \PY{o}{\PYZhy{}} \PY{n}{vec}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{vec}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{vec}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}
 \end{Verbatim}
 \normalsize
 
Before applying clustering methods, we visualize data on all possible pairplots. There are no obvious clusters in the two-dimensional visualization. 
    \begin{center}
	\adjustimage{max size={0.85\linewidth}{0.9\paperheight}}{output_8_1.png}
\end{center}
 


    Clusters might follow a categorical feature already present in data. A categorical attribute with the least amount of variants we have is the MPAA rating. We color the plots with accordance to the ratings to see how they divide our data. 
                
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
\end{center}
     

\subsection{K-means application}

The first method we apply to find the clusters is k-means at k=5. We take random initializations of 5 cluster centers and choose the best by k-means criteria from 10 initializations. The sum of squared distances from points to cluster centers, the K-means criterion: 13.613. 
  \footnotesize{  
    \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}16}]:} 
    \PY{n}{kmeans\PYZus{}k5} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}init}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{RANDOM\PYZus{}SEED}\PY{p}{)}
    \PY{n}{kmeans\PYZus{}k5}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{task\PYZus{}df}\PY{p}{)}
    \PY{n}{task\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels\PYZus{}k5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{kmeans\PYZus{}k5}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{task\PYZus{}df}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sum of squared distances from points to cluster centers, k=5:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kmeans\PYZus{}k5}\PY{o}{.}\PY{n}{inertia\PYZus{}}\PY{p}{)}
    \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{task\PYZus{}df}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels\PYZus{}k5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{vars}\PY{o}{=}\PY{n}{quant\PYZus{}features}\PY{p}{)}
    \end{Verbatim}
} \normalsize

We visualize the obtained clusters using pairplots. They  don't match to MPAA\_Rating directly, but divide the data into reasonable descriptive categories.


    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_2.png}
\end{center}
 


We would specially distinguish cluster 3 (purple colored), the old films, mostly not long and with small amount of deaths. Cluster 4 (yellow colored) is not large and contains movies with the highest number of deaths, they differ in length, and are relatively recent ones. 

Now we apply K-means at  k=9 and get a lot of smaller clusters.  Sum of squared distances from points to cluster centers, k=9: 10.079.

 
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_2.png}
\end{center}


Some clusters are obviously inherited from the pevious ones: new cluster 8 follows cluster 3 with the higest body count, the new cluster 0 follows the discussed cluster 3 (with old films) from the previous plot. Besides these two there are many small clusters at the main body of the films, overlapping on most  pairplots, without potent interpretation. We conclude that 9 clusters are too many for this dataset, and 5 clusters describe the data better: the cluster boundaries are clearer and clusters are reasonably interpretable.

\subsection{Interpretation}

For the rest of this task we consider the partition with k=5. First we explore how movies are destributed among clusters, minding their MPAA ratings (the correspondence of the ratings is in application \ref{MPAA}). 

           
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_1.png}
\end{center}

The plot shows the amount of movies of each rating in each cluster. For example we see that 165 R movies are in cluster 2. Clusters 0, 1, 2 aggregate the most part of the movies, mostly with R and PG-13 ratings. That may show that we have mostly adult films in the dataset. That is not a surprise due to considered subject. The number of films at the clusters are: 
2 - 233,
1  -  122,
0  -  119,
3  -   55,
4  -   16.

Next we compare the mean values of features between the clusters. The following barplot shows the mean body count per clusters, separated by ratings. The height of each bar shows the average body count of movies of a certain rating in a cluster. The vertical black bar represents the spread - the highest point shows the maximum, the lowest point shows the minimum.


            
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
\end{center}
 

It's clear that cluster 4 contains the movies with the highest body count, as was supposed in the cluster interpretation. It contains the PG-13 movie with the highest body count in the dataset (836): \textit{"Lord of the Rings: Return of the King"}. The highest point of the black bars shows this movie. 

Next we consider the average length of the movies by clusters: 
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_1.png}
\end{center}
 
This plot is not very informative. We can see that clusters 1 and 2 are on avearage the shortest ones. And the movies from the cluster 4, with highest body count, are long on average. 

Next we consider the release years of movies by cluster and ratings.

\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
\end{center}
 
It can be seen that the cluster 3 contains mostly old movies, as was discovered before. Now we can see that these are mostly pre-1980 movies. Cluster 1 contains movies from about the 90-s.
And it is interesting that the highest body count movies, captured by cluster 4, are mostly recent - post-2010.

Moreover, from this diagram it can be seen that for some ratings there are films only from one-two clusters, mostly from the third one, capturing old films. These ratings were used in previous years, but then were renamed or changed. As there are only several such movies in our dataset, we do not think it influence any results.


\subsection{Bootstrap}

In this part of the paper we inspect two clusters: 1 and 3. As was mentioned, cluster 3 contains mostly old, pre-1980 movies and cluster 1 has movies from the 90-s.

First, we consider length and release year, and then we focus on our main feature, the body count. From the scatter plot the difference between the clusters can be clearly seen.
            
\begin{center}
	\adjustimage{max size={0.6\linewidth}{0.9\paperheight}}{output_17_1.png}
\end{center}
 

We inspect the distribution of our primary feature, body count, in the two clusters.
\footnotesize   \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cluster\PYZus{}0}\PY{p}{[}\PY{n}{target\PYZus{}feature}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cluster\PYZus{}1}\PY{p}{[}\PY{n}{target\PYZus{}feature}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{cluster\PYZus{}0}\PY{o}{.}\PY{n}{Body\PYZus{}Count}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{cluster\PYZus{}1}\PY{o}{.}\PY{n}{Body\PYZus{}Count}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\normalsize
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_2.png}
\end{center}
 
The parameters of the distributions are listed in the table. The distributions seem to be different but we cannot be sure as the samples are rather small. 

\begin{center}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Parameter} & \textbf{Cluster 1}  & \textbf{Cluster 3}  \\ \hline
		count & 122.000000 & 55.000000  \\ \hline
		mean  & 56.795082  & 105.654545 \\ \hline
		std   & 47.931414  & 87.318323  \\ \hline
		min   & 1.000000   & 4.000000   \\ \hline
		25\%  & 20.000000  & 44.500000  \\ \hline
		50\%  & 42.500000  & 91.000000  \\ \hline
		75\%  & 87.250000  & 147.000000 \\ \hline
		max   & 258.000000 & 471.000000 \\ \hline
	\end{tabular}
\end{center}

To compare the distribution between the clusters we apply bootstrap in both pivotal and non-pivotal versions with 5000 samples. We obtain that  the distributions of Body\_Count really differ between these clusters. They not only have different means, as evident by confidence intervals, but also have different bell shapes. 

We provide the following histogram plots for bootstrap sample means. Green lines show pivotal $95\%$ confidence intervals, red lines show $95\%$ non-pivotal confidence intervals. It it interesting that pivotal and non-pivotal confidence intervals are nearly identical. From that we conclude that the distribution of the feature is approximately Gaussian.


\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_2.png}
\end{center}

\begin{center}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Parameter} & \textbf{Cluster 1}  & \textbf{Cluster 3}  \\ \hline
		mean & 56.844 & 105.385 \\ \hline
		pivotal & (48.317, 65.371) & (82.949, 127.823)  \\ \hline
		non-pivotal & (48.474, 65.590)  & (83.672, 128.711)  \\ \hline
	\end{tabular}
\end{center}

The code for bootstrap implementation and the confidence intervals is below. We use pivotal and non-pivotal bootstrap for the confidence intervals.

\footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k}{def} \PY{n+nf}{bootstrap\PYZus{}sample}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{:}
\PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{vec}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{bootstrap\PYZus{}means}\PY{p}{(}\PY{n}{srs}\PY{p}{,} \PY{n}{\ \ \ \ sample\PYZus{}amount}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}\PY{p}{:}
\PY{n}{\ \ \ \ samples\PYZus{}ix} \PY{o}{=} \PY{n}{bootstrap\PYZus{}sample}\PY{p}{(}\PY{n}{srs}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{sample\PYZus{}amount}\PY{p}{)}\PY{o}{.}\PY{n}{T}
\PY{n}{\ \ \ \ means} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{srs}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{sample}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{sample} \PY{o+ow}{in} \PY{n}{samples\PYZus{}ix}\PY{p}{]}\PY{p}{)}
\PY{k}{\ \ \ \ return} \PY{n}{means}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{b\PYZus{}means\PYZus{}cluster\PYZus{}0} \PY{o}{=} \PY{n}{bootstrap\PYZus{}means}\PY{p}{(}\PY{n}{cluster\PYZus{}0}\PY{o}{.}\PY{n}{Body\PYZus{}Count}\PY{p}{)}
\PY{n}{b\PYZus{}means\PYZus{}cluster\PYZus{}1} \PY{o}{=} \PY{n}{bootstrap\PYZus{}means}\PY{p}{(}\PY{n}{cluster\PYZus{}1}\PY{o}{.}\PY{n}{Body\PYZus{}Count}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{confidence\PYZus{}interval\PYZus{}pivotal}\PY{p}{(}\PY{n}{vec}\PY{p}{)}\PY{p}{:}
\PY{n}{    mean} \PY{o}{=} \PY{n}{vec}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n}{    std} \PY{o}{=} \PY{n}{vec}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{k}{    return} \PY{p}{[}\PY{n}{mean}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.96}\PY{o}{*}\PY{n}{std}\PY{p}{,} \PY{n}{mean}\PY{o}{+}\PY{l+m+mf}{1.96}\PY{o}{*}\PY{n}{std}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{confidence\PYZus{}interval\PYZus{}non\PYZus{}pivotal}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{alpha}\PY{p}{)}\PY{p}{:}
\PY{n}{    left} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{100}\PY{o}{\PYZhy{}}\PY{n}{alpha}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{    right} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{alpha}\PY{o}{+}\PY{p}{(}\PY{l+m+mi}{100}\PY{o}{\PYZhy{}}\PY{n}{alpha}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{)}
\PY{k}{    return} \PY{p}{[}\PY{n}{left}\PY{p}{,} \PY{n}{right}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{distplot\PYZus{}with\PYZus{}conf\PYZus{}intervals}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
\PY{k}{    if} \PY{o+ow}{not} \PY{n}{ax}\PY{p}{:}
\PY{n}{        ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
\PY{k}{    for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{confidence\PYZus{}interval\PYZus{}pivotal}\PY{p}{(}\PY{n}{vec}\PY{p}{)}\PY{p}{:}
\PY{n}{        line} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{x}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{    line}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pivotal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{    for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{confidence\PYZus{}interval\PYZus{}non\PYZus{}pivotal}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{l+m+mi}{95}\PY{p}{)}\PY{p}{:}
\PY{n}{        line} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{x}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{    line}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{non\PYZhy{}pivotal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{    ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{    return} \PY{n}{sns}\PY{o}{.}\PY{n}{distplot}\PY{p}{(}\PY{n}{vec}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{,} \PY{n}{norm\PYZus{}hist}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}


\end{Verbatim}


\normalsize


Next we build the 95\% confidence intervals for the grand mean of the feature by using bootstrap. Also we use bootstrap to compare cluster 0 to the grand mean.  We obtain:


\begin{center}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Parameter} & \textbf{Cluster 1}  & \textbf{Grand mean}  \\ \hline
		mean & 56.844 & 72.157 \\ \hline
		pivotal & (48.317, 65.371) & (64.374, 79.34)  \\ \hline
		non-pivotal & (48.474, 65.590)  & (64.65, 80.238)  \\ \hline
	\end{tabular}
\end{center}

\normalsize

Comparison between the grand mean and cluster 1:
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_2.png}
\end{center}
We pay attention here to the fact that the bell shape of cluster 1 body count resembles the grand mean bell shape closely.



    \section{Contingency Table Analysis}
    
In this part of the paper we consider the contingency tables. We take three nominal features from the dataset, producing them by splitting from the quantitative features. Consider the histograms for IMDB\_Rating, Year and Body\_count:

\newpage
\begin{multicols}{2}
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_4.png}
	IMDB\_Rating
\end{center}
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5.png}
	Year
\end{center}
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6.png}
	 Body\_count
\end{center}

\end{multicols}

To make the features we split all three attributes with reasonable visible thresholds, shown on the plots. We take 4 intervals for each attribute. For the IMDB rating we choose 6, 7 and 8 as thresholds, as they are caught by the histogram. It can be really well interpreted as very bad, bad, good and very good films. For the Year we choose 1980, 1990, 2000, 2005 as thresholds. In the previous part we had the cluster of old films, which appeared before 1980, and we can use  it here to set the thresholds. All the others were chosen to make the groups approximately equal. For the on-screen deaths we have chosen 20, 50, 120. 

\footnotesize
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{categorize\PYZus{}func}\PY{p}{(}\PY{n}{split\PYZus{}points}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{categorize}\PY{p}{(}\PY{n}{param}\PY{p}{)}\PY{p}{:}
        \PY{n}{split\PYZus{}points}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
        \PY{k}{if} \PY{p}{(}\PY{n}{param} \PY{o}{\PYZlt{}} \PY{n}{split\PYZus{}points}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZlt{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{split\PYZus{}points}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{k}{for} \PY{n}{index} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{split\PYZus{}points}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{param} \PY{o}{\PYZlt{}} \PY{n}{split\PYZus{}points}\PY{p}{[}\PY{n}{index} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{:}
                \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{≥}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{split\PYZus{}points}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{)}
        \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{≥}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{split\PYZus{}points}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{n}{categorize}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{bodies} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Body\PYZus{}Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{categorize\PYZus{}func}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{imdb\PYZus{}rating} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IMDB\PYZus{}Rating}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{categorize\PYZus{}func}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{5.0}\PY{p}{,} \PY{l+m+mf}{7.0}\PY{p}{,} \PY{l+m+mf}{8.0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{year} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{categorize\PYZus{}func}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1990}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{,} \PY{l+m+mi}{2005}\PY{p}{]}\PY{p}{)}\PY{p}{)}
  
  \end{Verbatim}
 
\normalsize

Next we build the contingency tables for the determined features:

\footnotesize
    \begin{Verbatim}[commandchars=\\\{\}]    
 \PY{n}{contingency\PYZus{}table\PYZus{}imdb\PYZus{}bodies} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{bodies}\PY{p}{,} \PY{n}{imdb\PYZus{}rating}\PY{p}{,} \PY{n}{margins}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \textbackslash 
                                 \PY{n}{margins\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{n}{contingency\PYZus{}table\PYZus{}year\PYZus{}bodies} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{bodies}\PY{p}{,} \PY{n}{year}\PY{p}{,} \PY{n}{margins}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{margins\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    
\end{Verbatim}
\footnotesize

	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		IMDB\_Rating  & \textless{}6.0 & $\geq$6.0 & $\geq$7.0 & $\geq$8.0 & Total \\
		Body\_Count   &                &      &      &      &             \\ \hline 
		\textless{}20 & 28             & 65   & 47   & 22   & 162     \\ \hline
		$\geq$120          & 14             & 20   & 44   & 14   & 92    \\ \hline
		$\geq$20           & 28             & 40   & 39   & 23   & 130    \\ \hline
		$\geq$50           & 39             & 49   & 52   & 21   & 161     \\ \hline
		Total         & 109            & 174  & 182  & 80   & 545  \\    \hline
	\end{tabular}
\vspace{0.2cm}

	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		Year          & \textless{}1980 & $\geq$1980 & $\geq$1990 & $\geq$2000 & $\geq$2005 & Total \\ \hline
		Body\_Count   &                 &       &       &       &       &       \\ \hline
		\textless{}20 & 12              & 14    & 39    & 44    & 53    & 162   \\ \hline
		$\geq$120          & 20              & 10    & 13    & 24    & 25    & 92    \\ \hline
		$\geq$20           & 10              & 23    & 48    & 19    & 30    & 130   \\ \hline
		$\geq$50           & 23              & 25    & 39    & 30    & 44    & 161   \\ \hline
		Total         & 65              & 72    & 139   & 117   & 152   & 545  \\ \hline
	\end{tabular}
\vspace{0.2cm}

  \normalsize 
   
 The first table shows the joint distribution of the IMDB rating and the number of deaths.  The second shows the   joint distribution between the Year and the number of deaths.
 
 \subsection{Relative probabilities tables}
 
 To see the relative probabilities we build the relative frequency tables over the same features: 
 
 \footnotesize
     \begin{Verbatim}[commandchars=\\\{\}]
 \PY{k}{def} \PY{n+nf}{make\PYZus{}relative}\PY{p}{(}\PY{n}{probabilites\PYZus{}df}\PY{p}{)}\PY{p}{:}
     \PY{n}{res} \PY{o}{=} \PY{n}{probabilites\PYZus{}df}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
     \PY{n}{res}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
     \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{res}\PY{p}{:}
         \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{:}
             \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{probabilites\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]}
     \PY{k}{return} \PY{n}{res}
 
 \PY{k}{def} \PY{n+nf}{normalize}\PY{p}{(}\PY{n}{contingency\PYZus{}df}\PY{p}{)}\PY{p}{:}
     \PY{n}{res} \PY{o}{=} \PY{n}{contingency\PYZus{}df}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
     \PY{n}{res} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
     \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{res}\PY{p}{:}
         \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{:}
             \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{contingency\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
     \PY{k}{return} \PY{n}{res}
 make_relative(normalize(contingency_table_imdb_bodies)) * 100
 \end{Verbatim}
   \normalsize 



The table for the rating and the number of deaths: 
\\
\footnotesize

\begin{tabular}{|l|l|l|l|l|}
	\hline
	IMDB\_Rating  & \textless{}6.0 & \geq 6.0      & \geq 7.0      & \geq 8.0      \\ \hline
	Body\_Count   &                &           &           &           \\ \hline \hline
	\textless{}20 & 17.283951      & 40.123457 & 29.012346 & 13.580247 \\ \hline \hline
\geq 20           & 21.538462      & 30.769231 & 30.000000 & 17.692308 \\ \hline \hline
	\geq 50           & 24.223602      & 30.434783 & 32.298137 & 13.043478 \\ \hline \hline
\geq 120          & 15.217391      & 21.739130 & 47.826087 & 15.217391 \\ \hline        \hline
\end{tabular}
\\
\normalsize

We can conclude  that a film with more than 200 body count probably will be well appreciated: it has probability nearly 48\% to have 7 and 22\% to have 6, while the film with <20 deaths will most probably have 6 (40\%) and less probably 7 (29\%). 
 
The table for the year and the number of deaths: 
\\
\footnotesize

\begin{tabular}{|l|l|l|l|l|l|}
	\hline
	Year          & \textless{}1980 & \geq 1980     & \geq 1990     & \geq 2000     & \geq 2005     \\ \hline
	Body\_Count   &                 &           &           &           &           \\ \hline
	\textless{}20 & 7.407407        & 8.641975  & 24.074074 & 27.160494 & 32.716049 \\ \hline
	\geq 20           & 7.692308        & 17.692308 & 36.923077 & 14.615385 & 23.076923 \\ \hline
	\geq 50           & 14.285714       & 15.527950 & 24.223602 & 18.633540 & 27.329193 \\ \hline
	\geq 120          & 21.739130       & 10.869565 & 14.130435 & 26.086957 & 27.173913 \\ \hline
\end{tabular}
\\
\normalsize

We see that if a movie has high amount of deaths it is most probably a new one or a very old one. And, as we have much more new films in the dataset, the probability for each cathegory of deaths increases with year (except one cell). 
   
 \subsection{Quetelet index tables} 
   
Now we build Quetelet index tables:
\footnotesize
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{make\PYZus{}quetelet}\PY{p}{(}\PY{n}{probabilites\PYZus{}df}\PY{p}{)}\PY{p}{:}
    \PY{n}{res} \PY{o}{=} \PY{n}{probabilites\PYZus{}df}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{res}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{res}\PY{p}{:}
        \PY{k}{for} \PY{n}{row} \PY{o+ow}{in} \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{:}
            \PY{n}{res}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]}\PY{o}{=}\PY{p}{(}\PY{n}{probabilites\PYZus{}df}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]}\PY{p}{/}\PY{n}{probabilites\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{row}\PY{p}{]} \PY{o}{\PYZhy{}} \textbackslash          
                      \PY{n}{probabilites\PYZus{}df}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{probabilites\PYZus{}df}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{k}{return} \PY{n}{res}
make_quetelet(normalize(contingency_table_imdb_bodies)) * 100
make_quetelet(normalize(contingency_table_year_bodies)) * 100
        \end{Verbatim}

   \begin{tabular}{|l|l|l|l|l|}
   	\hline
   	IMDB\_Rating  & \textless{}6.0 & $\geq$6.0       & $\geq$7.0       & $\geq$8.0       \\ \hline
   	Body\_Count   &                &            &            &            \\ \hline
   	\textless{}20 & -13.580247     & 25.674046  & -13.122371 & -7.484568  \\ \hline
   	$\geq$120          & -23.913043     & -31.909045 & 43.215480  & 3.668478   \\ \hline
   	$\geq$20           & 7.692308       & -3.625111  & -10.164835 & 20.528846  \\ \hline
   	$\geq$50           & 21.118012      & -4.672664  & -3.283052  & -11.141304 \\ \hline
   \end{tabular}
   
     \normalsize 
   
 The table shows the relative probability compared to the average.   We can conclude that аilms that have 120 or more bodies are more appreciated: they have odds to have rating more than 8.0 $\approx$4\% more than average and chance that ф film will have rating less than 6.0 is $\approx$23\% less than average.
   
\footnotesize

 \begin{tabular}{|l|l|l|l|l|l|}
 	\hline
 	Year          & \textless{}1980 & $\geq$1980      & $\geq$1990      & $\geq$2000      & $\geq$2005      \\ \hline
 	Body\_Count   &                 &            &            &            &            \\ \hline
 	\textless{}20 & -37.891738      & -34.585048 & -5.608846  & 26.516830  & 17.304256  \\ \hline
 	$\geq$120          & 82.274247       & -17.723430 & -44.596497 & 21.516165  & -2.567220  \\ \hline
 	$\geq$20           & -35.502959      & 33.920940  & 44.770338  & -31.919790 & -17.257085 \\ \hline
 	$\geq$50           & 19.780220       & 17.537957  & -5.022566  & -13.202739 & -2.010461  \\ \hline
 \end{tabular}  
   
\normalsize 

This is an quetelet table for year and the number of deaths. We conclude that the films with the highest amount of deaths have probability $\approx$82\% more than average to be released before 1980. At the same time films with less than 20 deaths are most probably released after 2000. 


\subsection{Chi-square \& Quetelet index }

We apply Pearson's chi-squared test to measure correlation between two variables. The value is the same as Summary Quetelet index. It shows the average relative increase in the prediction of the number ofthe number of deaths in the movie, when the year or the IMDB rating becomes known.

First, we find expected number of films in each category under assumption of independence of features. Also we find number of degrees of freedom. Let's consider the Year and the Body\_Count:

   \footnotesize \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{obs\PYZus{}values\PYZus{}year\PYZus{}bodies} \PY{o}{=} \PY{n}{trunc\PYZus{}total}\PY{p}{(}\PY{n}{contingency\PYZus{}table\PYZus{}year\PYZus{}bodies}\PY{p}{)}
\PY{n}{exp\PYZus{}values\PYZus{}year\PYZus{}bodies} \PY{o}{=} \PY{n}{expected\PYZus{}values}\PY{p}{(}\PY{n}{contingency\PYZus{}table\PYZus{}year\PYZus{}bodies}\PY{p}{)}
\PY{n}{degrees\PYZus{}of\PYZus{}freedom\PYZus{}year\PYZus{}bodies} \PY{o}{=} \PY{n}{reduce}\PY{p}{(}\PY{n}{mul}\PY{p}{,} \PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \textbackslash
                                       \PY{n}{obs\PYZus{}values\PYZus{}year\PYZus{}bodies}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{ddof\PYZus{}year\PYZus{}bodies} \PY{o}{=} \PY{n}{obs\PYZus{}values\PYZus{}year\PYZus{}bodies}\PY{o}{.}\PY{n}{size} \PY{o}{\PYZhy{}} \PY{n}{degrees\PYZus{}of\PYZus{}freedom\PYZus{}year\PYZus{}bodies} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
\end{Verbatim} 
\normalsize


The tables for observed frequencies and expected under the independence condition:

\footnotesize \begin{tabular}{|l|l|l|l|l|l|}
		\hline
	Year          & \textless{}1980 & $\geq$1980 & $\geq$1990 & $\geq$2000 & $\geq$2005 \\ \hline
	Body\_Count   &                 &       &       &       &       \\ \hline
	\textless{}20 & 12              & 14    & 39    & 44    & 53    \\ \hline
	$\geq$120          & 20              & 10    & 13    & 24    & 25    \\ \hline
	$\geq$20           & 10              & 23    & 48    & 19    & 30    \\ \hline
	$\geq$50           & 23              & 25    & 39    & 30    & 44    \\ \hline
\end{tabular} \normalsize

\vspace{0.2cm}
\footnotesize \begin{tabular}{|l|l|l|l|l|l|}
		\hline
	Year          & \textless{}1980 & $\geq$1980     & $\geq$1990     & $\geq$2000     & $\geq$2005     \\ \hline
	Body\_Count   &                 &           &           &           &           \\ \hline
	\textless{}20 & 19.321101       & 21.401835 & 41.317431 & 34.777982 & 45.181651 \\ \hline
	$\geq$120          & 10.972477       & 12.154128 & 23.464220 & 19.750459 & 25.658716 \\ \hline
	$\geq$20           & 15.504587       & 17.174312 & 33.155963 & 27.908257 & 36.256881 \\ \hline
	$\geq$50           & 19.201835       & 21.269725 & 41.062385 & 34.563303 & 44.902752 \\ \hline
\end{tabular} \normalsize

\vspace{0.2cm}

The obtained degrees of freedom amount for $\chi^2$ test is 12.

The $\chi^2$ test for the hypothesis of independence between year and number of bodies:

    \footnotesize \begin{Verbatim}[commandchars=\\\{\}]
 \PY{n}{scipy}\PY{o}{.}\PY{n}{stats}\PY{o}{.}\PY{n}{chisquare}\PY{p}{(}\PY{n}{trunc\PYZus{}total}\PY{p}{(}\PY{n}{contingency\PYZus{}table\PYZus{}year\PYZus{}bodies}\PY{p}{)}\PY{p}{,} \textbackslash
        \PY{n}{expected\PYZus{}values}\PY{p}{(}\PY{n}{contingency\PYZus{}table\PYZus{}year\PYZus{}bodies}\PY{p}{)}\PY{p}{,} \PY{n}{ddof}\PY{o}{=}\PY{n}{ddof\PYZus{}year\PYZus{}bodies}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
\end{Verbatim} 
\normalsize

We can see that critical pvalue is less than $10^{-4}$. So we can reject the hypothesis of independence between year and number of deaths with level of confidence 99.99\%.


Now consider the same tables for the other feature, IMDB rating, with number of bodies. Observed frequencies and expected under the independence condition:

\footnotesize \begin{tabular}{|l|l|l|l|l|}
		\hline
	IMDB\_Rating  & \textless{}6.0 & $\geq$6.0 & $\geq$7.0 & $\geq$8.0 \\ \hline
	Body\_Count   &                &      &      &      \\ \hline
	\textless{}20 & 28             & 65   & 47   & 22   \\ \hline
	$\geq$120          & 14             & 20   & 44   & 14   \\ \hline
	$\geq$20           & 28             & 40   & 39   & 23   \\ \hline
	$\geq$50           & 39             & 49   & 52   & 21   \\ \hline
\end{tabular}

 \normalsize
 \vspace{0.2cm}
\footnotesize \begin{tabular}{|l|l|l|l|l|}
		\hline
	MDB\_Rating   & \textless{}6.0 & $\geq$6.0      & $\geq$7.0      & $\geq$8.0      \\ \hline
	Body\_Count   &                &           &           &           \\ \hline
	\textless{}20 & 32.4           & 51.721101 & 54.099083 & 23.779817 \\ \hline
	$\geq$120          & 18.4           & 29.372477 & 30.722936 & 13.504587 \\ \hline
	$\geq$20           & 26.0           & 41.504587 & 43.412844 & 19.082569 \\ \hline
	$\geq$50           & 32.2           & 51.401835 & 53.765138 & 23.633028 \\ \hline
\end{tabular}
 \normalsize

The number of degrees of freedom is 9, and it is expectedly less than in previous case as we have less categories for rating. 

The $\chi^2$ test results for the hypothesis of independence between the IMDB rating and number of bodies:

\footnotesize \begin{Verbatim}[commandchars=\\\{\}]
 Power\_divergenceResult(statistic=18.231013449960972, pvalue=0.03258602359739147)
\end{Verbatim} 
\normalsize

  The critical pvalue here is about 3\%. So we can reject the hypothesis of
independence with level of confidence 95\%, but we cannot reject the
hypothesis with level of confidence 97\% or higher.


Now we compute chi-square summary quetelet index tables for both features:
\\

\footnotesize \begin{tabular}{|l|l|l|l|l|} \hline
	IMDB\_Rating  & \textless{}6.0 & Total     & $\geq$6.0      & $\geq$7.0      \\ \hline \hline
	Body\_Count   &                &           &           &           \\ \hline
	\textless{}20 & -3.802469      & 5.071541  & 16.688130 & -6.167515 \\ \hline
	Total         & 3.239576       & 18.231013 & 6.566671  & 7.175824  \\ \hline
	$\geq$120          & -3.347826      & 9.798763  & -6.381809 & 19.014811 \\ \hline
	$\geq$20           & 2.153846       & 1.461151  & -1.450044 & -3.964286 \\ \hline
	$\geq$50           & 8.236025       & 1.899559  & -2.289605 & -1.707187 \\ \hline
\end{tabular} 
\normalsize

\vspace{0.2cm}

\footnotesize \begin{tabular}{|l|l|l|l|l|l|l|} \hline
	Year          & \textless{}1980 & Total     & $\geq$1980     & $\geq$1990     & $\geq$2000     & $\geq$2005     \\ \hline \hline
	Body\_Count   &                 &           &           &           &           &           \\ \hline
	\textless{}20 & -4.547009       & 9.262296  & -4.841907 & -2.187450 & 11.667405 & 9.171256  \\ \hline
	Total         & 12.906996       & 39.298444 & 5.572056  & 11.545967 & 6.805703  & 2.467722  \\ \hline
	$\geq$120          & 16.454849       & 13.407037 & -1.772343 & -5.797545 & 5.163880  & -0.641805 \\ \hline
	$\geq$20           & -3.550296       & 14.499397 & 7.801816  & 21.489762 & -6.064760 & -5.177126 \\ \hline
	$\geq$50           & 4.549451        & 2.129715  & 4.384489  & -1.958801 & -3.960822 & -0.884603 \\ \hline
\end{tabular} 
\normalsize
\\
The quetelet index is the sum of the values in the table. We compute it to make sure that it is equal to $\chi^2$ statistics. 

 
   
    \section{PCA: Hidden Factor \& Data visualization}
    
In this part of the paper we apply principal component analysis. We choose three features to take into consederation. First one is the Body\_Count, as the main feature of interest. The second one is Year, as we know from the previous analysis that they are well connected. The third one is IMDB rating. 

First we center and normalize the features. There are two ways of normalization, by standard deviation and by range:

    \footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{normalize\PYZus{}range}\PY{p}{(}\PY{n}{vec}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{n}{vec} \PY{o}{\PYZhy{}} \PY{n}{vec}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{vec}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{vec}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{k}{def} \PY{n+nf}{normalize\PYZus{}std}\PY{p}{(}\PY{n}{vec}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{n}{vec} \PY{o}{\PYZhy{}} \PY{n}{vec}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{vec}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim} 
 \normalsize

Recall the plots of joint and self distributions of the features: 

    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1_4.png}
\end{center}
    
First we apply PCA and inspect the data scatter explained by each PC. We apply PCA to data centered and normalized by range, as proper PCA application requires data to be on the same scale. 



\footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pc\PYZus{}range} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{p}{)}
\PY{n}{pc\PYZus{}range}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{task\PYZus{}df\PYZus{}normalized\PYZus{}range}\PY{p}{)}
\PY{n}{scatter} \PY{o}{=} \PY{n}{data\PYZus{}scatter}\PY{p}{(}\PY{n}{task\PYZus{}df\PYZus{}normalized\PYZus{}range}\PY{p}{)}
\PY{n}{contibs} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{p}{[}\PY{n}{pc\PYZus{}contrib}\PY{p}{(}\PY{n}{s}\PY{p}{)} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{pc\PYZus{}range}\PY{o}{.}\PY{n}{singular\PYZus{}values\PYZus{}}\PY{p}{]}\PY{p}{)}
\PY{n}{contibs\PYZus{}rel} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{p}{[}\PY{n}{pc\PYZus{}contrib\PYZus{}rel}\PY{p}{(}\PY{n}{s}\PY{p}{,} \PY{n}{scatter}\PY{p}{)} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{pc\PYZus{}range}\PY{o}{.}\PY{n}{singular\PYZus{}values\PYZus{}}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data scatter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scatter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Contributions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{contibs}\PY{p}{,} \PY{n}{contibs\PYZus{}rel}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Raw PC contribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{contibs}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{contibs}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Relative PC contribution, }\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{contibs\PYZus{}rel}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{contibs\PYZus{}rel}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim} 
 \normalsize

   The contributions of the components to the explanation of data scatter in distribution are:

    \footnotesize
\begin{tabular}{lll}
	\# of Component & Raw contribution                     & Relative contribution \\ \hline
	0            & 21.672266                             & 0.545363              \\
	1            & 11.444299                             & 0.287985              \\
	2            & 6.622589  & 0.166651             
\end{tabular}
\normalsize

So, the first PC explains 55\% of data scatter, the second one explains 29\%, the third one 17\%. The diagram illustrates the numbers from the table.

    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_7_2_4.png}
\end{center}

\subsection{Visualization}

We will use K-means ($k = 5$) cluster labels from part 2 to see how PCA helps vizualize them.
	
We apply PCA to data with clusters centered by mean and normalized by range and plot the first two principal components.

    \footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pc\PYZus{}range} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{pca\PYZus{}result} \PY{o}{=} \PY{n}{pc\PYZus{}range}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{task\PYZus{}df\PYZus{}normalized\PYZus{}range}\PY{p}{)}
\PY{n}{pca\PYZus{}result} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{pca\PYZus{}result}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pc1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pc2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{pca\PYZus{}result}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels\PYZus{}k5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{labels\PYZus{}k5}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pc1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pc2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fit\PYZus{}reg}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels\PYZus{}k5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pca\PYZus{}result}\PY{p}{)}
\end{Verbatim} 
 \normalsize

    \begin{center}
	\adjustimage{max size={0.7\linewidth}{0.9\paperheight}}{output_10_2_4.png}
\end{center}    
    
Evidently the clusters are easily separatable, except for 0 and 4, which tend to overlap with other clusters.

Next we apply PCA to data normalized by standard deviation. The code is similar, the resulting plot is as follows:

   \begin{center}
	\adjustimage{max size={0.7\linewidth}{0.9\paperheight}}{output_11_2_4.png}
\end{center}


This one is much less descriptive, clusters overlap alot. 

Next, we implement and apply conventional PCA. 
    

    \footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{conv\PYZus{}pca}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n}{X} \PY{o}{=} \PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{n}{covariance\PYZus{}matrix} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

    \PY{n}{eigvals}\PY{p}{,} \PY{n}{eigvectors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{covariance\PYZus{}matrix}\PY{p}{)}
    \PY{n}{order} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{eigvals}\PY{p}{)}
    \PY{n}{ind1}\PY{p}{,} \PY{n}{ind2} \PY{o}{=} \PY{n}{order}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} top 2 max eigvalues indices}
    \PY{n}{eigvectors} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{eigvectors}\PY{o}{.}\PY{n}{T}
    \PY{n}{pc1} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigvectors}\PY{p}{[}\PY{n}{ind1}\PY{p}{]}\PY{p}{)}  \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{eigvals}\PY{p}{[}\PY{n}{ind1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n}{pc2} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigvectors}\PY{p}{[}\PY{n}{ind2}\PY{p}{]}\PY{p}{)}  \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{eigvals}\PY{p}{[}\PY{n}{ind2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{pc1}\PY{p}{,} \PY{n}{pc2}
\end{Verbatim} 
\normalsize

Firstly we apply conventional PCA to  data normalized by range:

    \begin{center}
	\adjustimage{max size={0.7\linewidth}{0.7\paperheight}}{output_14_2_4.png}
\end{center}

    The results are nearly identical to those of model-based PCA. That is expected. When using conventional PCA it's important to center our data (while model-based PCA can be applied to non-centered data). Here the data is centered, so we get same results.
    
Secondly we apply  the conventional PCA to data normalized by standard deviation. It also gives identical results, but rotated 180 degrees: 


 \begin{center}
	\adjustimage{max size={0.7\linewidth}{0.7\paperheight}}{output_15_2_4.png}
\end{center}

    
    \subsection{Hidden factor analysis}
    
In this part we try to uncover the hidden factor behind selected features and determine which of them contribute most. In this function we implement applying PCA, extracting the first singular triplet and rescaling it: 

    
\footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}feature\PYZus{}weights}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{n}{u}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{vh} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{svd}\PY{p}{(}\PY{n}{X}\PY{p}{)}
    \PY{n}{z} \PY{o}{=} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{u}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
    \PY{n}{sigma} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{s}\PY{p}{)}
    \PY{n}{c} \PY{o}{=} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{vh}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
    \PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{n}{c}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    \PY{n}{feature\PYZus{}weights} \PY{o}{=} \PY{n}{c}\PY{o}{*}\PY{n}{alpha}
    \PY{n}{contrib} \PY{o}{=} \PY{n}{sigma}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{n}{data\PYZus{}scatter}\PY{p}{(}\PY{n}{X}\PY{p}{)}
    \PY{k}{return} \PY{n}{feature\PYZus{}weights}\PY{p}{,} \PY{n}{contrib}

\PY{n}{weights}\PY{p}{,} \PY{n}{contrib} \PY{o}{=} \PY{n}{get\PYZus{}feature\PYZus{}weights}\PY{p}{(}\PY{n}{task\PYZus{}df}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{task\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{contrib}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{weights}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
['Body\_Count', 'IMDB\_Rating', 'Year']
99.78585508347525 [ 3.5 96.4  0.1]
\end{Verbatim} 
 \normalsize

So, on the 0-100 rank we obtain that the first principal  component explains 99.8\% of data scatter. The most contributing feature is IMDB\_Rating, which determines 96.4\% of the PC. 


    
    
    
    \section{2D regression}

In this part of the paper we examine the 2D regressions. First, we choose two variables to take into consideration. All the scatter-plots are presented on the picture. 

    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_4_1_5.png}
\end{center}

None of these are "linear-like" plots. We choose Length\_Minutes and IMDB\_Rating as the best options. First, we explore their scatter-plot closer.

    \begin{center}
	\adjustimage{max size={0.6\linewidth}{0.9\paperheight}}{output_3_1_5.png}
\end{center}

Based on the visualization, we expect positive correlation here. 
We build a regression of Length\_Minutes to IMDB\_Rating. 
    
    
    \footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
 \PY{n}{training\PYZus{}x} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Length\PYZus{}Minutes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
 \PY{n}{training\PYZus{}y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IMDB\PYZus{}Rating}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
 \PY{n}{classificator} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{training\PYZus{}x}\PY{p}{,} \PY{n}{training\PYZus{}y}\PY{p}{)}
 \PY{n}{predicted\PYZus{}y} \PY{o}{=} \PY{n}{classificator}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{training\PYZus{}x}\PY{p}{)}
 \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{training\PYZus{}x}\PY{p}{,} \PY{n}{training\PYZus{}y}\PY{p}{)}
 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{training\PYZus{}x}\PY{p}{,} \PY{n}{predicted\PYZus{}y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
 classificator.coef_
\end{Verbatim} 
 \normalsize

The visualization of the regression line:
    
    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_1_5.png}
\end{center}
  
The slope of the regression is 0.0211439. That means that every minute of a film gives additional 0.0211439 to rating. It doesn't seem much, but these 0.02 may raise the film on tens of places in the search or in the top-movies rating, as there are >5 mln movies on IMDB rated at ten-point scale.

This correlation may follow the suggested hypothesis: the production of the long movie usually 
requires a bigger budget, which will be approved if the film is supposed to be good. So, long films are generally among the good ones.


The determinacy coefficient and mean absolute error:
  

\footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classificator}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{training\PYZus{}x}\PY{p}{,} \PY{n}{training\PYZus{}y}\PY{p}{)}
{\color{outcolor}Out[{\color{outcolor}14}]:} 0.18600394479373705
\PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{training\PYZus{}y}\PY{p}{,} \PY{n}{predicted\PYZus{}y}\PY{p}{)}
{\color{outcolor}Out[{\color{outcolor}15}]:} 0.7823580398705626
\end{Verbatim} 
 \normalsize  
  

The determinacy coefficient is 0.186, which is very low value. At the same time mean absolute error is very high at 0.782. It shows that the regression model works really bad on these features. 

Nevertheless, we try to predict values of rating by the length of movie. We take four values for the length of the movie: [80, 100, 150, 200] minutes. The predicted ratings are: [6.0716766 , 6.49455604, 7.55175466, 8.60895327].

    \footnotesize    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{lengths\PYZus{}to\PYZus{}predict} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{60}\PY{p}{,} \PY{l+m+mi}{90}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{]}\PY{p}{)}
\PY{n}{prdicted\PYZus{}ratings} \PY{o}{=} \PY{n}{classificator}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{lengths\PYZus{}to\PYZus{}predict}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

\end{Verbatim} 
 \normalsize

 
\begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_1_5.png}
\end{center}


\section{Conclusion}

In this project we explore the data on movies. The first section contains the descriprion of the dataset and some hypothesis. In the next section we apply the clustering methods to analyze the data. The third section contains the contingency table analysis, $\chi^2$ and quetelet index. In the next we study the hidden factors by principal component analysis. Then, we use 2D regression to find some more patterns. 

The main feature of the consideration was the number of on-screen deaths in the movies. The higest number in our data is 836 \textit{("Lord of the Rings: Return of the King")}. 
There are several things of great interest among the patterns and correlations that we found. The old films, which were filmed before 1980 stay aside from the others  as they are mostly long and highly-rated. Modern films, of course, have a higher amount of deaths. And the films with a lot of violence are rated with stronger restrictions by MPAA. 

PC analysis revealed the high ability of IMDB rating to describe the other numerical features, as it is the main contributor to the first component. That shows that the ratings really reflect some important properties and summarize them.


% To our knowledge, this paper is the most miscellaneous and contributive analysis of this dataset. Although there are a few statistical descriptions on the patterns of the set.


    
    \section{Applications}
    
\begin{enumerate}
\item \textbf{MPAA Ratings interpretation. }\label{MPAA}
Sourse: \href{https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system#From_M_to_GP_to_PG}{MPAA}
\begin{itemize}
\item \textbf{G — General audiences}. All ages admitted. Nothing that would offend parents for viewing by children.
\item \textbf{PG – Parental Guidance Suggested}. Some material may not be suitable for children. Parents urged to give "parental guidance". May contain some material parents might not like for their young children.
\item \textbf{M:} Suggested for Mature Audiences – parental discretion advised (before 1984). The same as modern PG. 
\item \textbf{GP} All Ages Admitted – Parental Guidance Suggested (before 1972). Renamed to PG.
\item \textbf{PG-13 – Parents Strongly Cautioned}. Some material may be inappropriate for children under 13. Parents are urged to be cautious. Some material may be inappropriate for pre-teenagers.
\item \textbf{R – Restricted}. Under 17 requires accompanying parent or adult guardian. Contains some adult material. Parents are urged to learn more about the film before taking their young children with them.
\item \textbf{NC-17 – Adults Only. }No One 17 and Under Admitted. Clearly adult. Children are not admitted.
\item \textbf{X}. No one under 17 admitted (before 1984). Was renamed to NC-17
\end{itemize}

\end{enumerate}


 
\end{document}